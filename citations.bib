@misc{scellier2021deeplearningtheoryneural,
      title={A deep learning theory for neural networks grounded in physics}, 
      author={Benjamin Scellier},
      year={2021},
      eprint={2103.09985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.09985}, 
}

@article{dillavou2022demonstration,
  title={Demonstration of decentralized physics-driven learning},
  author={Dillavou, Sam and Stern, Menachem and Liu, Andrea J and Durian, Douglas J},
  journal={Physical Review Applied},
  volume={18},
  number={1},
  pages={014040},
  year={2022},
  publisher={APS}
}

@article{altman2024experimental,
  title={Experimental demonstration of coupled learning in elastic networks},
  author={Altman, Lauren E and Stern, Menachem and Liu, Andrea J and Durian, Douglas J},
  journal={Physical Review Applied},
  volume={22},
  number={2},
  pages={024053},
  year={2024},
  publisher={APS}
}

@article{wright2022deep,
  title={Deep physical neural networks trained with backpropagation},
  author={Wright, Logan G and Onodera, Tatsuhiro and Stein, Martin M and Wang, Tianyu and Schachter, Darren T and Hu, Zoey and McMahon, Peter L},
  journal={Nature},
  volume={601},
  number={7894},
  pages={549--555},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{pashine2019directed,
  title={Directed aging, memory, and nature’s greed},
  author={Pashine, Nidhi and Hexner, Daniel and Liu, Andrea J and Nagel, Sidney R},
  journal={Science advances},
  volume={5},
  number={12},
  pages={eaax4215},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{gold2019self,
  title={Self-organized novelty detection in driven spin glasses},
  author={Gold, Jacob M and England, Jeremy L},
  journal={arXiv preprint arXiv:1911.07216},
  year={2019}
}

@article{kedia2019drive,
  title={Drive-specific adaptation in disordered mechanical networks of bistable springs},
  author={Kedia, Hridesh and Pan, Deng and Slotine, Jean-Jacques and England, Jeremy L},
  journal={arXiv preprint arXiv:1908.09332},
  year={2019}
}

@article{berneman2024designing,
  title={Designing precise dynamical steady states in disordered networks},
  author={Berneman, Marc and Hexner, Daniel},
  journal={arXiv preprint arXiv:2409.05060},
  year={2024}
}

@article{stern2021supervised,
  title={Supervised learning in physical networks: From machine learning to learning machines},
  author={Stern, Menachem and Hexner, Daniel and Rocks, Jason W and Liu, Andrea J},
  journal={Physical Review X},
  volume={11},
  number={2},
  pages={021045},
  year={2021},
  publisher={APS}
}

@article{scellier2017equilibrium,
  title={Equilibrium propagation: Bridging the gap between energy-based models and backpropagation},
  author={Scellier, Benjamin and Bengio, Yoshua},
  journal={Frontiers in computational neuroscience},
  volume={11},
  pages={24},
  year={2017},
  publisher={Frontiers Media SA}
}

@article{stern2023learning,
  title={Learning without neurons in physical systems},
  author={Stern, Menachem and Murugan, Arvind},
  journal={Annual Review of Condensed Matter Physics},
  volume={14},
  number={1},
  pages={417--441},
  year={2023},
  publisher={Annual Reviews}
}

@article{stern2020continual,
  title={Continual learning of multiple memories in mechanical networks},
  author={Stern, Menachem and Pinson, Matthew B and Murugan, Arvind},
  journal={Physical Review X},
  volume={10},
  number={3},
  pages={031044},
  year={2020},
  publisher={APS}
}

@article{FRENCH1999128,
title = {Catastrophic forgetting in connectionist networks},
journal = {Trends in Cognitive Sciences},
volume = {3},
number = {4},
pages = {128-135},
year = {1999},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(99)01294-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661399012942},
author = {Robert M. French},
keywords = {Catastrophic forgetting, Connectionist networks, Connectionism, Memory, Learning, Interference},
abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically’. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.}
}

@article{fisher1936use,
  title={The use of multiple measurements in taxonomic problems},
  author={Fisher, Ronald A},
  journal={Annals of eugenics},
  volume={7},
  number={2},
  pages={179--188},
  year={1936},
  publisher={Wiley Online Library}
}